# %%

# FRAUD DETECTION PIPELINE ‚Äî BUSINESS & RISK ORIENTED APPROACH
# Context:
# Este projeto simula um cen√°rio real de uma fintech / institui√ß√£o de pagamentos,
# cujo objetivo √© identificar transa√ß√µes com alto risco de fraude.

# Do ponto de vista de neg√≥cio, fraudes geram:
# - perdas financeiras diretas
# - risco operacional
# - impacto em liquidez e funding
# - deteriora√ß√£o da experi√™ncia do cliente

# O pipeline est√° estruturado nas seguintes etapas:
# 1) Entendimento do problema de risco
# 2) An√°lise explorat√≥ria orientada a comportamento financeiro
# 3) Prepara√ß√£o dos dados para modelagem
# 4) Benchmark de modelos supervisionados
# 5) Ajuste de threshold com base em custo de erro
# 6) Conclus√µes para tomada de decis√£o


# FASE 1 ‚Äî DEFINI√á√ÉO DO PROBLEMA DE RISCO

# O objetivo do projeto √© identificar transa√ß√µes com comportamento an√¥malo
# que indiquem potencial fraude.
# Em um contexto real, esse tipo de modelo apoia decis√µes como:
# - bloqueio preventivo de transa√ß√µes
# - acionamento de regras antifraude
# - prioriza√ß√£o de an√°lises manuais

# A modelagem considera padr√µes relacionados a:
# Comportamento do cliente
# Comportamento do merchant
# Valores transacionados fora do padr√£o
# Concentra√ß√£o temporal de transa√ß√µes

# Tudo isso √© importante para evitar preju√≠zos, detectar anomalias antes que se tornem fraudes reais, antecipar riscos em merchants e categorias.

#FASE 2 - CARREGAMENTO + AN√ÅLISE EXPLORAT√ìRIA DOS DADOS 

# %%
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

# Caminho relativo esperado para o dataset
DATA_PATH = "data/transactions.csv"

df = pd.read_csv(DATA_PATH)
print(df.head())


# %%
df.info()
#Podemos notar que algumas colunas categ√≥ricas v√™m como strings, inclusive colunas que parecem num√©ricas como age e zip codes. 
#√â interessante padronizar e codificar essas vari√°veis antes de modelar, porque o modelo n√£o vai conseguir interpret√°-las corretamente se continuarem como texto.

# %%
df.describe()
#Aqui, avaliamos alguns valores para buscar comportamentos, poss√≠veis erros e outliers
# A distribui√ß√£o de step indica um horizonte temporal completo que pode ser explorado para detectar sazonalidade de risco e picos de transa√ß√£o.
#Pela an√°lise na coluna amount, identificamos uma assimetria muito forte, indicando a presen√ßa de outliers


# %%
df["fraud"].value_counts(normalize=True)
# A vari√°vel target (fraud) √© altamente desbalanceada.
# Apenas ~1.2% das transa√ß√µes s√£o fraudes.

# Do ponto de vista de neg√≥cio, isso significa que:
# - Accuracy n√£o √© uma m√©trica confi√°vel
# - O custo de um falso negativo (fraude n√£o detectada) √© alto

# Por isso, m√©tricas como Recall e AUC s√£o priorizadas, pois refletem melhor a capacidade do modelo em capturar risco.

# %%
#Distribui√ß√£o dos valores transacionados

sns.histplot(df["amount"], bins=50)
plt.title("Distribui√ß√£o de Valores")
plt.show()

#Plotando um gr√°fico histograma para observar a distribui√ß√£o dos amounts, mostrando graficamente que existem outliers

# %%
#Calculando o volume de transa√ß√µes no tempo
#Esse gr√°fico mostra picos e vales e serve para antecipar picos de demanda, garantir liquidez e entender sazonalidade

df.groupby("step")["amount"].sum().plot(figsize=(12,4))
plt.title("Fluxo de Transa√ß√µes ao Longo do Tempo")
plt.show()

#A an√°lise do gr√°fico mostra um aumento no n√∫mero de transa√ß√µes com o passar do tempo, com oscila√ß√µes mantendo um padr√£o
#Tamb√©m observamos muitos picos e valores rapidos, sem quedas ou altas muito bruscas.
#Essa oscila√ß√£o pode representar comportamento heterogeneo dos merchants e/ou muitos clientes fazendo transa√ß√µes simultaneas


# %%
#C√°lculo do volume por categoria

df["category"].value_counts().plot(kind="bar")
plt.title("Transa√ß√µes por Categoria")
plt.show()


# %%
#FASE 3: Etapa de pr√© processamento

# Nesta etapa, preparamos os dados para garantir que o modelo consiga capturar padr√µes de risco de forma consistente.
# Vari√°veis categ√≥ricas s√£o transformadas em num√©ricas, permitindo que o modelo aprenda rela√ß√µes entre categorias, como:
# Tipo de transa√ß√£o, canal e comportamento do merchant.

# Uma c√≥pia do dataset original foi mantida para garantir rastreabilidade e evitar perda de informa√ß√£o.

import sklearn 
from sklearn.preprocessing import LabelEncoder

df_prep = df.copy()
cat_cols = df_prep.select_dtypes(include="object").columns
cat_cols

for col in cat_cols:
    df_prep[col] = LabelEncoder().fit_transform(df_prep[col])


#Garantindo que a coluna target fraud √© num√©rica:

df_prep["fraud"] = df_prep["fraud"].astype(int)

#Remover colunas que n√£o queremos usar no modelo
cols_to_remove = ["gender", "customer", "merchant","zipcodeOri", "zipMerchant", "age"]
df_prep = df_prep.drop(columns=cols_to_remove)

# Algumas colunas s√£o removidas por n√£o agregarem valor preditivo ou por introduzirem risco de overfitting.

# IDs como customer e merchant n√£o representam comportamento, apenas identifica√ß√£o.
# Vari√°veis como g√™nero, idade e zipcode foram removidas por n√£o serem determinantes diretas de fraude neste contexto e para evitar vieses indesejados.


# %%
#Separa√ß√£o das features e da vari√°vel alvo (target)
#Primeiro, removemos a coluna "fraud" para que ela n√£o entre como feature no modelo e armazenamos em X
#Agora, X tem todas as colunas num√©ricas e categorizadas, exceto fraud
#Em resumo:
#X ‚Üí valor, hor√°rio, merchant, categoria, idade, etc.
#y ‚Üí fraude ou n√£o fraude

X = df_prep.drop("fraud", axis=1)
y = df_prep["fraud"]

# %%
#Verificando se o n√∫mero de linhas √© igual, se X tem todas as features esperadas e se y tem apenas 1 coluna:
X.shape, y.shape

# %%
#DATA SPLIT - TRAIN/TEST

#Divis√£o dos dados em treino e teste
#Estamos dividindo as features (X) e target (y) em conjuntos de treino e teste
#80% dos dados v√£o treinar o modelo e 20% v√£o avaliar a performance real
#Usamos stratify=y para garantir que a propor√ß√£o de fraudes e n√£o fraudes seja preservada nos dois conjuntos
#Isso √© fundamental para evitar vi√©s, j√° que datasets de fraude tendem a ser altamente desbalanceados
#O random_state torna a divis√£o reprodut√≠vel, o que permite comparar resultados de forma consistente independente de quando ou onde rodar o c√≥digo

from sklearn.model_selection import train_test_split

X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, stratify=y, random_state=42
)

# %%

#FASE 4: BENCHMARK DE MODELOS SUPERVISIONADOS
#Queremos saber qual √© o modelo ideal a ser usado para supervisionamento.
#Primeiro, importamos os modelos e m√©tricas

from sklearn.linear_model import LogisticRegression
from sklearn.ensemble import RandomForestClassifier
from sklearn.svm import SVC
from sklearn.neighbors import KNeighborsClassifier
from xgboost import XGBClassifier
from lightgbm import LGBMClassifier

from sklearn.metrics import (
    accuracy_score,
    precision_score,
    recall_score,
    f1_score,
    roc_auc_score,
    confusion_matrix
)

import pandas as pd

# %%
#Depois, criamos um dicionario com todos os modelos para benchmark

models = {
    "Logistic Regression": LogisticRegression(max_iter=3000, class_weight="balanced"),
    "Random Forest": RandomForestClassifier(n_estimators=300, class_weight="balanced"),
    "KNN": KNeighborsClassifier(n_neighbors=5),
    "XGBoost": XGBClassifier(
        n_estimators=300,
        learning_rate=0.05,
        max_depth=6,
        subsample=0.8,
        colsample_bytree=0.8,
        eval_metric="logloss"
    ),
    "LightGBM": LGBMClassifier(
        n_estimators=300,
        learning_rate=0.05,
        class_weight="balanced"
    )
}

# %%
#Definimos uma fun√ß√£o para treinar e avaliar cada modelo

def evaluate_model(model, X_train, y_train, X_test, y_test):

    # Treinar
    model.fit(X_train, y_train)

    # Previs√µes
    y_pred = model.predict(X_test)
    y_proba = model.predict_proba(X_test)[:, 1] if hasattr(model, "predict_proba") else None

    # M√©tricas
    results = {
        "Accuracy": accuracy_score(y_test, y_pred),
        "Precision": precision_score(y_test, y_pred, zero_division=0),
        "Recall": recall_score(y_test, y_pred),
        "F1-score": f1_score(y_test, y_pred),
        "AUC": roc_auc_score(y_test, y_proba) if y_proba is not None else None,
        "Confusion Matrix": confusion_matrix(y_test, y_pred)
    }

    return results

# %%
#E por fim rodamos todos os modelos para comparar

results_table = []

for name, model in models.items():
    
    print(f"\nüîç Treinando modelo: {name}...\n")
    metrics = evaluate_model(model, X_train, y_train, X_test, y_test)

    row = {
        "Modelo": name,
        "Accuracy": metrics["Accuracy"],
        "Precision": metrics["Precision"],
        "Recall": metrics["Recall"],
        "F1-score": metrics["F1-score"],
        "AUC": metrics["AUC"]
    }

    results_table.append(row)

    print("Matriz de Confus√£o:")
    print(metrics["Confusion Matrix"])
    print("-" * 60)

# DataFrame final com os resultados
results_df = pd.DataFrame(results_table)
results_df


# %%
#Conclus√µes sobre o benchmark

# Nesta etapa, comparamos diferentes modelos supervisionados para entender o trade-off entre:
#- capacidade de detec√ß√£o de fraude (recall)
#- custo operacional de falsos positivos

# O objetivo n√£o √© apenas maximizar m√©tricas, mas escolher um modelo vi√°vel em produ√ß√£o.

#Entre elas, XGBoost tem maior recall, seguido por random forest e depois KNN, o qual n√£o performa bem em grandes datasets
#Logo, em conclus√£o, os melhores modelos a serem usados s√£o XGBoost ou Random Forest
#XGBoost: lida bem com problemas complexos e tamb√©m desbalanceados, tem alta performance e controle fino de hiperpar√¢metros
#Random Forest n√£o ser√° utilizado pois pode ser lento com datasets grandes, e foi lento no benchmark em quest√£o
#Portanto, o modelo utilizado ser√° XGBoost.

# %%
#Confeccionando o modelo XGBoost ap√≥s o benchmark

from xgboost import XGBClassifier


# %%
model_xgb = XGBClassifier(
    n_estimators=300,
    learning_rate=0.05,
    max_depth=6,
    subsample=0.8,
    colsample_bytree=0.8,
    eval_metric="logloss"
)


# %%
model_xgb.fit(X_train, y_train)


# %%
y_pred = model_xgb.predict(X_test)


# %%
y_proba = model_xgb.predict_proba(X_test)[:, 1]


# %%
from sklearn.metrics import classification_report, confusion_matrix, roc_auc_score

print(classification_report(y_test, y_pred))
print(confusion_matrix(y_test, y_pred))
print("AUC:", roc_auc_score(y_test, y_proba))


# %%
#Fase 5: Ajuste de threshold com base em custo de erro

#Os resultados nos mostram √≥timo valor de f1-score, o que indica bons valores de precis√£o e recall.
#Podemos ajustar o valor de threshold para aumentar o recall, visto que isso muda o comportamento do modelo
# O ajuste de threshold √© uma decis√£o de neg√≥cio.

# Threshold menor:
# - aumenta recall
# - reduz fraudes n√£o detectadas
# - aumenta custo operacional (falsos positivos)

# Threshold maior:
# - reduz interven√ß√µes
# - aumenta risco financeiro

# Neste contexto, priorizamos recall, pois o custo da fraude √© maior que o custo do falso positivo.

# %%
#Agora, vamos testar alguns valores de threshold, entre 0.3 a 0.7 para observar  qual √© o ideal
#Calcularemos precis√£o, recall e F1 para cada threshold
#Tamb√©m mostraremos graficamente como precision e recall mudam conforme alteramos threshold


# %%
y_proba = model_xgb.predict_proba(X_test)[:, 1]

import numpy as np
from sklearn.metrics import precision_score, recall_score, f1_score

thresholds = np.arange(0.30, 0.71, 0.05)  # thresholds de 0.30 a 0.70
results = []

for t in thresholds:
    y_pred_adj = (y_proba >= t).astype(int)

    precision = precision_score(y_test, y_pred_adj, zero_division=0)
    recall = recall_score(y_test, y_pred_adj)
    f1 = f1_score(y_test, y_pred_adj)

    results.append([t, precision, recall, f1])

# Mostrar resultados em tabela
import pandas as pd
threshold_df = pd.DataFrame(results, columns=["Threshold", "Precision", "Recall", "F1-score"])

threshold_df


# %%
#Gr√°fico de precision e recall de acordo com a varia√ß√£o de threshold

import numpy as np
import matplotlib.pyplot as plt
from sklearn.metrics import precision_recall_curve

# Gera√ß√£o das curvas de precision, recall e thresholds
precision, recall, thresholds = precision_recall_curve(y_test, y_proba)

# Criar figura
plt.figure(figsize=(10, 6))

# Plotar Precision e Recall em fun√ß√£o do Threshold
plt.plot(thresholds, precision[:-1], label='Precision', linewidth=2)
plt.plot(thresholds, recall[:-1], label='Recall', linewidth=2)

# Detalhes do gr√°fico
plt.title("Curva Precision x Recall x Threshold", fontsize=16)
plt.xlabel("Threshold", fontsize=14)
plt.ylabel("Valor", fontsize=14)
plt.grid(True, linestyle='--', alpha=0.6)
plt.legend(fontsize=12)

plt.show()


# %%
#Portanto, threshold igual a 0.3 √© o que apresentou melhor equil√≠brio entre as m√©tricas estudadas.
#Esse valor reduz falsos negativos sem comprometer muito os falsos positivos
#O gr√°fico acima mostra que o ponto de equil√≠brio entre precis√£o e recall fica pr√≥ximo de 0.25 √† 0.27
#√â importante ressaltar que esse ponto √© s√≥ uma refer√™ncia, e n√£o necessariamente maximiza nossa m√©trica mais importante (recall)

# %%
#Agora vamos ajustar o c√≥digo final do modelo XGBoost para threshold=0.3

# TREINAMENTO DO MODELO XGBOOST

from xgboost import XGBClassifier
from sklearn.metrics import (
    classification_report,
    confusion_matrix,
    roc_auc_score,
    precision_score,
    recall_score,
    f1_score
)

# 1) Criar e treinar o modelo
model_xgb = XGBClassifier(
    n_estimators=300,
    learning_rate=0.05,
    max_depth=6,
    subsample=0.8,
    colsample_bytree=0.8,
    eval_metric="logloss"
)

model_xgb.fit(X_train, y_train)

# PREVIS√ïES COM THRESHOLD PERSONALIZADO

# 2) Probabilidades da classe 1 (fraude)
y_proba = model_xgb.predict_proba(X_test)[:, 1]

# 3) Aplicar o threshold = 0.30
threshold = 0.30
y_pred_adj = (y_proba >= threshold).astype(int)

# AVALIA√á√ÉO DO MODELO AJUSTADO

# 4) M√©tricas finais
precision = precision_score(y_test, y_pred_adj)
recall = recall_score(y_test, y_pred_adj)
f1 = f1_score(y_test, y_pred_adj)
auc = roc_auc_score(y_test, y_proba)

print(f"Threshold utilizado: {threshold}")
print(f"Precision: {precision:.4f}")
print(f"Recall:    {recall:.4f}")
print(f"F1-score:  {f1:.4f}")
print(f"AUC:       {auc:.4f}")

# 5) Classification Report
print("\nClassification Report (com threshold ajustado):")
print(classification_report(y_test, y_pred_adj))

# 6) Matriz de confus√£o
print("Matriz de confus√£o (com threshold = 0.30):")
print(confusion_matrix(y_test, y_pred_adj))



